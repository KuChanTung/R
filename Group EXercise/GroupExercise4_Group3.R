#******* Group information *******

#Group :

#Please rename this file to Group_.R
#Please write your code in each block.

#******* end of Group information *******


#****** 1.Please load the dataset "winequality-red.csv" and do some data managements. *****
library(data.table)

# load data
wine <- data.table::fread("winequality-red.csv", data.table = F)

# basic understanding
str(wine)

# check na
colSums(is.na(wine))

# summary
summary(wine)

#***** end of 1. *****


#****** 2. Please try to discover some patterns among predictors using PCA. *****


wine2 <- data.frame(scale(as.matrix(wine))) 
attributes(scale(as.matrix(wine)))  # quality : scaled:center = 5.63602251, scaled:scale = 0.807569440


wine_scale<- wine2[, c(1:11)] # Center the data

# Variables correlation
cor_X = cor(wine_scale)
cor_X

# Find the high correlation variables
wine3 <- wine_scale[, c(1,3,6,7,8,9)]

# Check their covariance
cov_X = cov(wine3)
cov_X

# Eigen decomposition of the covariance matrix
eigen_quality = eigen(cov_X)
eigen_quality

# 1st PC with loadings
eigen_quality$vectors[, 1]

# 1st PC score
sum(eigen_quality$vectors[, 1] ^ 2) # sum of squared loadings is 1

# PCA using princomp()
pc_quality = princomp(wine3, scores = T)
# loadings of each PC
pc_quality$loadings
# scores of each PC
pc_quality$scores
# percentages of explained variance 
pc_quality$sdev ^ 2 / sum((pc_quality$sdev) ^ 2)
# A biplot should help you understand more about PCs
biplot(pc_quality)
# Better biplot ggbiplot::ggbiplot() 
# library("devtools"); install_github("vqv/ggbiplot")

#ggbiplot::ggbiplot(pc_quality, labels = rownames(X), labels.size = 5)
ggbiplot::ggbiplot(pc_quality,labels.size = 5)

ggbiplot::ggscreeplot(pc_quality)


wine_result <- data.frame(pc_quality$scores[,1:3], quality = scale(wine$quality))


#****** 3. Please use the new features generated by PCA to
# build a predictive model to predict the target "quality. *****


# fit a linear model
fit = lm(quality ~ ., data = wine_result)
summary(fit)


k_fold_CV_lm = function(f, d, k){
  numOfRec = nrow(d) # number of observations
  response_var = all.vars(f)[1] # name of the response variable
  rmse_vec = rep(0, k)
  sample_idx_k = rep(sample(1:k), round(numOfRec / k) + 1)[1:numOfRec]
  for(i in 1:k){
    m = lm(f, d[sample_idx_k != i, ]);
    pred = predict(m, newdata = d[sample_idx_k == i, ])
    
    # quality : scaled:center = 5.63602251, scaled:scale = 0.807569440
    tb = data.frame(actual = (d[[response_var]][sample_idx_k == i]) * 0.807569440 + 5.63602251, predict = pred * 0.807569440 + 5.63602251)
    rmse_vec[i] = sqrt(mean((tb$actual - tb$predict)^2))
  }
  return(sqrt(mean(rmse_vec^2)))
}

k_fold_CV_lm(quality ~ ., d = wine_result, 10)
# 10-fold RMSE = 0.7676643
#***** end of 3. *****